# LLM Configuration
LLM_PROVIDER=watsonx
MODEL_NAME=mistralai/mistral-medium-2505
TEMPERATURE=0.7

# Watsonx Configuration
WATSONX_API_KEY=your_watsonx_api_key_here
WATSONX_PROJECT_ID=your_watsonx_project_id_here
WATSONX_URL=https://us-south.ml.cloud.ibm.com

# Instana Configuration (for fetch_application tool)
INSTANA_BASE_URL=https://your-instana-instance.instana.io
INSTANA_API_TOKEN=your_instana_api_token_here

# Instana MCP Server Configuration (stdio mode)
INSTANA_MCP_SERVER_PATH=npx
INSTANA_MCP_SERVER_ARGS=-y @instana/mcp-server-instana

# OpenAI Configuration (uncomment and set to use OpenAI instead)
# LLM_PROVIDER=openai
# MODEL_NAME=gpt-4o-mini
# OPENAI_API_KEY=your_openai_api_key_here

# vLLM Configuration (uncomment and set to use vLLM instead)
# LLM_PROVIDER=vllm
# MODEL_NAME=meta-llama/Llama-2-7b-chat-hf
# VLLM_API_BASE=http://localhost:8000/v1
# VLLM_API_KEY=EMPTY
